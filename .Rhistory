dh1[[length(dh1)]] <- d_L
View(dh1)
View(dh2)
dh2[[length(dh2)]] = softmax(nn$h)
nn$h
softmax1 <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
softmax1 <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
ReLU <- function(h_j) {
# Applied the ReLU transform to each element in the list
return(pmax(0, h_j))
}
forward <- function(nn, inp){
# put the values for the first layer in each node
nn$h[[1]] <- as.numeric(inp)
# compute the remaining node values
for(i in 2:length(nn$h)) {
nn$h[[i]] <- ReLU(nn$W[[i-1]] %*% nn$h[[i-1]] + nn$b[[i-1]])
}
return(nn)
}
softmax <- function(class, h_final) {
# h_final is a list of raw values from the output node
return(exp(class)/sum(exp(h_final)))
}
softmax1 <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
W <- list(matrix(c(-2.5, -1.5, 0.6, 0.4), nrow = 2),
matrix(c(-0.1, 2.4, -2.2, 1.5, -5.2, 3.7), ncol = 2))
b <- list(matrix(c(1.6, 0.7), ncol = 1),
matrix(c(0, 0, 1), ncol = 1))
h <- list(matrix(c(0, 0), ncol = 1),
matrix(rep(0,2), ncol = 2),
matrix(rep(0,3), ncol = 1))
nn1 <- list(W = W, b = b, h = h)
nn <- forward(nn, inp = c(0.5, 0.37))
nn <- forward(nn1, inp = c(0.5, 0.37))
View(nn)
dW <- nn$W
dh1 <- nn$h
dh2 <- nn$h
db <- nn$b
k <- 1
nn_len <- length(nn$h[[length(nn$h)]])
d_L <- c(rep(0,nn_len))
# Iterate through the values in the final node
for (i in 1:nn_len){
if (i == k){
d_L[i] <- softmax(nn$h[[nn_len]][i], nn$h[[length(nn$h)]]) - 1
}
else{
d_L[i] <- softmax(nn$h[[nn_len]][i], nn$h[[length(nn$h)]])
}
}
dh1[[length(dh1)]] <- d_L
View(dh1)
View(dh2)
dh2[[length(dh2)]] = softmax1(nn$h[length(nn$h)])
nn$h[length(nn$h)]
exp(nn$h[length(nn$h)])
class(nn$h[length(nn$h)])
nn$h
nn$h[1]
class(nn$h[3])
unlist(nn$h[3])
class(unlist(nn$h[3]))
dh2[[length(dh2)]] = softmax1(unlist(nn$h[length(nn$h)]))
dh2[[length(dh2)]][k] = dh2[[length(dh2)]][k] - 1
View(dh2)
View(dh2)
View(nn1)
View(nn)
netup <- function(d) {
W <- lapply(seq_along(d)[-length(d)], function(i) {
matrix(runif(d[i] * d[i+1], 0, 0.2), d[i+1], d[i])
})
h <- lapply(seq_along(d), function(j) {
rep(0, times = d[j])
})
b <- lapply(seq_along(d)[-1], function(z) {
runif(d[z], 0, 0.2)
})
nn <- list(h = h, W = W, b = b)
return(nn)
}
d <- c(4,8,7,3)
set.seed(100)
nn <- netup(d)
View(nn)
ReLU <- function(h_j) {
# Applied the ReLU transform to each element in the list
return(pmax(0, h_j))
}
# The function forward() is for computing the every node value except on the
# first layer
# forward() takes on 2 inputs:
# (1) nn: a network list as returned by netup()
# (2) inp: a vector of input values for the first layer
# forward() returns the updated version of network list
forward <- function(nn, inp){
# put the values for the first layer in each node
nn$h[[1]] <- as.numeric(inp)
# compute the remaining node values
for(i in 2:length(nn$h)) {
nn$h[[i]] <- ReLU(nn$W[[i-1]] %*% nn$h[[i-1]] + nn$b[[i-1]])
}
return(nn)
}
nn1 <- forward(nn, training_data[1,-5])
# data is iris data
data <- iris
# divide the data into training and test data
# data[, ncol(data)] <- as.numeric(data[, ncol(data)])
test_data <- as.matrix(data[seq(5, nrow(data), by = 5),])
training_data <- as.matrix(data[-seq(5, nrow(data), by = 5),])
# k <- seq(nrow(unique(data[ncol(data)])))
k <- training_data[,ncol(training_data)]
inp <- training_data[,-ncol(training_data)]
nn1 <- forward(nn, training_data[1,-5])
View(nn1)
class(nn1$h[1])
class(nn1$h[[1]])
nn1$h[1]
nn1$h[[1]]
dh2[[3]]
dh2[[3]]
nn1$h[[3]]
nn1$h[[3]][2]
ReLU <- function(h_j) {
# Applied the ReLU transform to each element in the list
return(pmax(0, h_j))
}
forward <- function(nn, inp){
# put the values for the first layer in each node
nn$h[[1]] <- as.numeric(inp)
# compute the remaining node values
for(i in 2:length(nn$h)) {
nn$h[[i]] <- ReLU(nn$W[[i-1]] %*% nn$h[[i-1]] + nn$b[[i-1]])
}
return(nn)
}
softmax <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
W <- list(matrix(c(-2.5, -1.5, 0.6, 0.4), nrow = 2),
matrix(c(-0.1, 2.4, -2.2, 1.5, -5.2, 3.7), ncol = 2))
b <- list(matrix(c(1.6, 0.7), ncol = 1),
matrix(c(0, 0, 1), ncol = 1))
h <- list(matrix(c(0, 0), ncol = 1),
matrix(rep(0,2), ncol = 2),
matrix(rep(0,3), ncol = 1))
nn1 <- list(W = W, b = b, h = h)
nn <- forward(nn1, inp = c(0.5, 0.37))
dW <- nn$W
dh1 <- nn$h
dh2 <- nn$h
db <- nn$b
View(nn)
dh2[[length(dh2)]] = softmax1(nn$h[[length(nn$h)]])
dh2[[length(dh2)]] = softmax(nn$h[[length(nn$h)]])
dh2[[length(dh2)]][k] = dh2[[length(dh2)]][k] - 1
k <- 1
dh2[[length(dh2)]][k] = dh2[[length(dh2)]][k] - 1
View(dh2)
View(nn)
a <- nn$h[[3]]
exp(a[1])
exp(a)
sum(exp(a))
a[1]/sum(exp(a))
a[1]/sum(exp(a)) - 1
a[1]
a[2]/sum(exp(a))
a <- nn$h[[3]]
a
b <- softmax(a)
b
e<- exp(a)
se <- sum(e)
se
e
a[1]/e
se
a[1]/se
exp(a[1])/se
exp(a[1])/se - 1
exp(a[2])/se
exp(a[3])/se
for (i in 10:1){
cat(i)
}
for (i in 10:1){
print(i)
}
len(nn$h)
length(nn$h)
View(nn)
i <- 1
i <- 2
nn$h[[3]]
k <- 1
dh[[length(dh)]] = softmax(nn$h[[length(nn$h)]])
dh <- nn$h
k <- 1
dh[[length(dh)]] = softmax(nn$h[[length(nn$h)]])
dh[[length(dh)]][k] = dh[[length(dh)]][k] - 1
View(dh)
dh[[3]]
relu_der <- as.numeric(nn$h[[i+1]] > 0)
nn$h[[3]]
nn$h[[3]] > 0
relu_der
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dl1
nn$W[[2]]
dh[[i]] <- t(nn$W[[i]]) %*% dl1
t(nn$W[[i]]) %*% dl1
View(nn)
(-0.1*-0.7608586 + 2.4*0.5182486 + -2.2*0.2426100)
i <- 1
nn$h[2]
View(nn)
relu_der <- as.numeric(nn$h[[i+1]] > 0)
relu_der
dh[[2]]
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dl1
i
nn$W[[1]]
dh[[i]] <- t(nn$W[[i]]) %*% dl1
dh[[1]]
dl1
i <- 2
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dl1
dh[[3]]
db[[2]]
db[[i]] <- dl1
dl1
dh[[3]]
db[[2]]
nn$h[[2]]
dl1
dl1 %*% t(nn$h[[i]])
View(dh)
i <- 1
dl1 <- dh[[i+1]] * relu_der
dh[[2]]
nn$h[[2]]
relu_der
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
db[[i]] <- dl1
dW[[i]] <- dl1 %*% t(nn$h[[i]])
View(dh)
View(db)
View(dW)
ReLU <- function(h_j) {
# Applied the ReLU transform to each element in the list
return(pmax(0, h_j))
}
forward <- function(nn, inp){
# put the values for the first layer in each node
nn$h[[1]] <- as.numeric(inp)
# compute the remaining node values
for(i in 2:length(nn$h)) {
nn$h[[i]] <- ReLU(nn$W[[i-1]] %*% nn$h[[i-1]] + nn$b[[i-1]])
}
return(nn)
}
softmax <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
W <- list(matrix(c(-2.5, -1.5, 0.6, 0.4), nrow = 2),
matrix(c(-0.1, 2.4, -2.2, 1.5, -5.2, 3.7), ncol = 2))
b <- list(matrix(c(1.6, 0.7), ncol = 1),
matrix(c(0, 0, 1), ncol = 1))
h <- list(matrix(c(0, 0), ncol = 1),
matrix(rep(0,2), ncol = 2),
matrix(rep(0,3), ncol = 1))
nn1 <- list(W = W, b = b, h = h)
nn <- forward(nn1, inp = c(0.5, 0.37))
dW <- nn$W
dh <- nn$h
db <- nn$b
# fill in the last dh
k <- 1
dh[[length(dh)]] = softmax(nn$h[[length(nn$h)]])
dh[[length(dh)]][k] = dh[[length(dh)]][k] - 1
for(i in (len(nn$h)-1):1) {
# Compute the derivative of the ReLU function
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
# Update gradients for weights and biases
dW[[i]] <- dl1 %*% t(nn$h[[i]])
db[[i]] <- dl1
# # Compute the loss for the next iteration
# d_loss <- t(nn$W[[i]]) %*% dh
}
for(i in (length(nn$h)-1):1) {
# Compute the derivative of the ReLU function
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
# Update gradients for weights and biases
dW[[i]] <- dl1 %*% t(nn$h[[i]])
db[[i]] <- dl1
# # Compute the loss for the next iteration
# d_loss <- t(nn$W[[i]]) %*% dh
}
View(db)
View(dh)
View(dW)
View(nn)
backward <- function(nn, k){
# loss wrt weights
dW <- nn$W
# loss wrt node values at each layer
dh <- nn$h
# loss wrt biases
db <- nn$b
# Compute the derivative of the loss for k_i w.r.t. h^L_j
dh[[length(dh)]] <- softmax(nn$h[[length(nn$h)]])
dh[[length(dh)]][k] <- dh2[[length(dh2)]][k] - 1
# Iterate through the number of operations linking the layers together
# E.g., if we have a 4-8-7-3, then we have 3 links or number of layers - 1
# Backpropagate through the layers to obtain the derivatives
for(i in (length(nn$h)-1):1){
# mask
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
# Update gradients for weights and biases
dW[[i]] <- dl1 %*% t(nn$h[[i]])
db[[i]] <- dl1
}
nn <- list(h = nn$h, W = nn$W, b = nn$b, dh = dh, dW = dW, db = db)
return(nn)
}
n <- backward(nn, k = 1)
backward <- function(nn, k){
# loss wrt weights
dW <- nn$W
# loss wrt node values at each layer
dh <- nn$h
# loss wrt biases
db <- nn$b
# Compute the derivative of the loss for k_i w.r.t. h^L_j
dh[[length(dh)]] <- softmax(nn$h[[length(nn$h)]])
dh[[length(dh)]][k] <- dh[[length(dh)]][k] - 1
# Iterate through the number of operations linking the layers together
# E.g., if we have a 4-8-7-3, then we have 3 links or number of layers - 1
# Backpropagate through the layers to obtain the derivatives
for(i in (length(nn$h)-1):1){
# mask
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
# Update gradients for weights and biases
dW[[i]] <- dl1 %*% t(nn$h[[i]])
db[[i]] <- dl1
}
nn <- list(h = nn$h, W = nn$W, b = nn$b, dh = dh, dW = dW, db = db)
return(nn)
}
n <- backward(nn, k = 1)
View(h)
View(n)
iris
data <- iris[1:5,:]
data <- iris[1:5,]
View(data)
k <- data[ncol(data)]
View(k)
data <- iris[sample(nrow(iris),size = 10),]
View(data)
k <- as.numeric(data[ncol(data)])
k <- as.numeric(data[,ncol(data)])
random_rows <- sample(nrow(inp), size = mb)
eta <- 0.01
data <- iris[sample(nrow(iris),size = 10),]
k <- as.numeric(data[,ncol(data)])
inp <- data
mb <- 3
set.seed(10)
eta <- 0.01
data <- iris[sample(nrow(iris),size = 10),]
k <- as.numeric(data[,ncol(data)])
inp <- data
mb <- 3
View(data)
View(nn)
random_rows <- sample(nrow(inp), size = mb)
mini_batch <- inp[random_rows,]
View(n)
View(mini_batch)
data[8,]
data[7,]
data[6,]
# get data
set.seed(10)
eta <- 0.01
data <- iris[sample(nrow(iris),size = 10),c(2,4,5)]
k <- as.numeric(data[,ncol(data)])
inp <- data
mb <- 3
View(data)
random_rows <- sample(nrow(inp), size = mb)
mini_batch <- inp[random_rows,]
k_mb <- k[random_rows]
inp <- data[,-ncol(data)]
View(inp)
# get data
set.seed(10)
eta <- 0.01
data <- iris[sample(nrow(iris),size = 10),c(2,4,5)]
k <- as.numeric(data[,ncol(data)])
inp <- data[,-ncol(data)]
mb <- 3
random_rows <- sample(nrow(inp), size = mb)
mini_batch <- inp[random_rows,]
k_mb <- k[random_rows]
View(mini_batch)
ReLU <- function(h_j) {
# Applied the ReLU transform to each element in the list
return(pmax(0, h_j))
}
forward <- function(nn, inp){
# put the values for the first layer in each node
nn$h[[1]] <- as.numeric(inp)
# compute the remaining node values
for(i in 2:length(nn$h)) {
nn$h[[i]] <- ReLU(nn$W[[i-1]] %*% nn$h[[i-1]] + nn$b[[i-1]])
}
return(nn)
}
backward <- function(nn, k){
# loss wrt weights
dW <- nn$W
# loss wrt node values at each layer
dh <- nn$h
# loss wrt biases
db <- nn$b
# Compute the derivative of the loss for k_i w.r.t. h^L_j
dh[[length(dh)]] <- softmax(nn$h[[length(nn$h)]])
dh[[length(dh)]][k] <- dh[[length(dh)]][k] - 1
# Iterate through the number of operations linking the layers together
# E.g., if we have a 4-8-7-3, then we have 3 links or number of layers - 1
# Backpropagate through the layers to obtain the derivatives
for(i in (length(nn$h)-1):1){
# mask
relu_der <- as.numeric(nn$h[[i+1]] > 0)
# Update dh for the current layer
dl1 <- dh[[i+1]] * relu_der
dh[[i]] <- t(nn$W[[i]]) %*% dl1
# Update gradients for weights and biases
dW[[i]] <- dl1 %*% t(nn$h[[i]])
db[[i]] <- dl1
}
nn <- list(h = nn$h, W = nn$W, b = nn$b, dh = dh, dW = dW, db = db)
return(nn)
}
softmax <- function(h_final) {
return(exp(h_final)/sum(exp(h_final)))
}
W <- list(matrix(c(-2.5, -1.5, 0.6, 0.4), nrow = 2),
matrix(c(-0.1, 2.4, -2.2, 1.5, -5.2, 3.7), ncol = 2))
b <- list(matrix(c(1.6, 0.7), ncol = 1),
matrix(c(0, 0, 1), ncol = 1))
h <- list(matrix(c(0, 0), ncol = 1),
matrix(rep(0,2), ncol = 2),
matrix(rep(0,3), ncol = 1))
nn <- list(W = W, b = b, h = h)
View(nn)
all_nn <- rep(list(nn), mb)
# get data
set.seed(10)
eta <- 0.01
data <- iris[sample(nrow(iris),size = 10),c(2,4,5)]
k <- as.numeric(data[,ncol(data)])
inp <- data[,-ncol(data)]
mb <- 3
random_rows <- sample(nrow(inp), size = mb)
mini_batch <- inp[random_rows,]
k_mb <- k[random_rows]
View(data)
View(mini_batch)
all_nn <- rep(list(nn), mb)
View(all_nn)
j <- 1
all_nn[[j]] <- forward(all_nn[[j]], inp = mini_batch[j,])
mini_batch[1,]
View(mini_batch)
(3.4*(-2.5)+0.37*0.6)+1.6
(3.4*(-1.5)+0.37*0.4)+0.7
all_nn[[j]] <- backward(all_nn[[j]], k_mb[j])
k_mb[j]
